# LLM-Guided Kernel Configuration Generator

This experiment aims to integrate LLM into the Tensile/TensileLite Tuning workflow to generate the optimal GEMM kernels on AMD GPU more efficiently. The key idea is to request the LLM to suggest one or more sets of better kernel parameters based on the benchmark/profiling results of a user-provided kernel configuration and the information of targeting hardware.

## Motivation

In the traditional Tensile/TensileLite tuning process, users need to provide a config yaml following the [Tensile Benchmark Protocol](https://rocm.docs.amd.com/projects/Tensile/en/latest/src/conceptual/benchmarking.html) to construct a search space of the combination of candidate kernel parameters (`ForkParameters`) for all the specified problem sizes.

![Tensile tuning pass](../docs/images/tensile_tuning_pass.png)

Tensile implements exhaustive benchmarking to find the optimal kernel configuration from the search space for every specific GEMM size. In order not to miss the true optimal kernel, we have to specify a large number of candidates for variable fork parameters. This makes the tuning workflow very time-consuming.

Tensile v2 provides users the flexibility to manually interrupt the multiplicative series of fork parameters with additions to dramatically reduce the size of the search space. For example:

- `(8 WorkGroups) * (12 ThreadTiles) + (4 NumLoadsCoalesedAs)` is supported by only v2
- `(8 WorkGroups) * (12 ThreadTiles) * (4 NumLoadsCoalesedAs)` is supported by both v1 and v2

However, reducing the redundant forking with this new feature while preserving the optimal solution highly depends on experts' knowledge.

## Objectives

Our goal is to build a framework that integrates an LLM into the Tensile/TensileLite tuning workflow to automatically generate improved kernel tuning configurations. This reduces manual effort and accelerates the search for optimal GEMM kernels on AMD GPUs.

### Stage 1: Search Space Pruning (Working in Progress)

Starting from a broad set of candidate kernel parameters, the LLM analyzes the configuration and profiling/benchmark data to remove unnecessary forks while preserving the true optimal kernel as a candidate. The outcome is a more compact config file that retains optimal solutions and significantly reduces TensileLite tuning time.

### Stage 2: Search Space Expansion (Not Started)

Starting from a limited set of candidate parameters, the LLM can recommend operations that expand the kernel-parameter search space by leveraging hardware specifications, problem details, and profiling/benchmark data. By proposing new parameter values and combinations not present in the original input, the generated configuration can introduce kernel variants that may achieve better performance than the initial candidates.

## Evaluation Metrics

To evaluate the effectiveness of LLM-guided optimization, we define three key metrics:

**Setup**: Let $\mathbb{K}_0$ be the original kernel set from the input config, and $\mathbb{K}_1$ be the optimized set generated by the LLM.

### Tuning Time Reduction

Measures the net time savings compared to baseline Tensile tuning, accounting for LLM generation overhead:

$$
TR = \frac{T(\mathbb{K}_0) - (T(\mathbb{K}_1) + \Lambda)}{T(\mathbb{K}_0)}
$$

where:
- $T(\mathbb{K})$ is Tensile tuning time for kernel set $\mathbb{K}$
- $\Lambda$ is LLM generation time (overhead)

A positive $TR$ indicates the LLM optimization saves time overall.

### Performance Retention

Measures whether the optimized set contains kernels with equivalent performance:

$$
PR = \frac{\max_{K_1\in\mathbb{K}_1} P(K_1)}{\max_{K_0\in\mathbb{K}_0} P(K_0)}
$$

where $P(K)$ is kernel performance measured in GFLOPS.

A value of $PR = 1.0$ means the best optimized kernel achieves the same performance as the best original kernel; $PR < 1.0$ indicates performance loss.

### Winner Consistency

Measures the stability of LLM recommendations across repeated runs:

$$
WC = \frac{1}{M}\sum_{i=1}^{M}\mathbf{1}(\text{Winner}(\mathbb{K}_0) = \text{Winner}(\mathbb{K}_1))
$$

where:
- $M$ is number of independent LLM runs
- $\text{Winner}(\mathbb{K}) = \argmax_{K\in\mathbb{K}}P(K)$ is the best-performing kernel from set $\mathbb{K}$

A higher $WC$ indicates the LLM reliably identifies the same optimal kernel across runs.

## Proposed Solution

### Input/Output Format

The input of LLM is a comprehensive prompt including the following contents:

- terminology definitions (both hardware parameters and kernel parameters)
- hardware specifications of targeting AMD GPU
- problem description (e.g. matrix size, data type, transpose)
- initial kernel configuration
- benchmark/profiling results of previous kernel configurations (after the initial stage)

The output is a modified kernel configuration that guides Tensile to find the optimal kernel for specific problem sizes on target hardware.

### Iterative Tuning

We proposed an iterative tuning procedure to get an optimal configuration. The pseudocode is shown below:

```
Input:
    model, num_iters, term_def, gpu_spec, problem_desc, config_yaml

Output:
    optimized_config_yaml

Subroutine IterativeTuning:
    kern_config = read_yaml(config_yaml)
    bench_result = call_tensile(kern_config)

    best_config = kernel_config
    best_perf = bench_result

    for i in [1 .. num_iters]:
        prompt = get_prompt(term_def, gpu_spec, problem_desc, kern_config, bench_result)
        advice = get_llm_response(model, prompt)

        kern_config = modify_config(kern_config, advice)
        bench_result = call_tensile(kern_config)

        if bench_result > best_perf:
            best_perf = bench_result
            best_config = kern_config

    return best_config, best_perf
```

### LLM Operations

At each iteration, the LLM proposes edits to the kernel configuration to steer Tensile toward better-performing kernels. It can perform the following types of operations:

- Remove a multiplicative fork parameter
- Remove an independent fork parameter
- Remove a fork parameter candidate
- Add a multiplicative fork parameter
- Add an independent fork parameter
- Add a fork parameter candidate

## Environment Setup

Install [Ollama](https://ollama.com/) in the Docker container.

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

Serve Ollama in the background (e.g. `tmux`, `nohup`)

```shell
# with tmux
apt update && apt install -y tmux
tmux new -ds ollama 'ollama serve'

# or with nohup
nohup ollama serve &> /tmp/ollama.log &
```

Download model in another session, take [Llama3.1-8B](https://ollama.com/library/llama3.1) for example.

```shell
ollama pull llama3.1:8b
```

Navigate to the project root (`ml_tuning/`), install this project as a package with its dependencies.

```shell
cd ..  # ml_tuning
pip install -e .
```

## CLI Usage

Run `confgen` with `--help` option to see the usage:

```shell
confgen --help
```

### Generate Modified Config YAML with LLM

You can see the usage of `generate` subcommand via:

```shell
confgen generate --help
```
Ensure the Ollama server is running before generating a modified config file. Then run:

```shell
confgen generate config.yaml output.md
```

To use a different model, add the `--model` option. For example:

```shell
confgen generate --model codellama:7b config.yaml output.md
```

### Generate Logic YAML with LLM

To generate a logic YAML, you have to provide an additional logic file with `--logic-yaml` as the example of output format for LLM. Also, make sure the Ollama server is running before you run the command.

```shell
confgen generate --logic-yaml logic.yaml config.yaml output.md
```

### Run Tensile Tuning

The `tensile` subcommand calls the `Tensile.sh` to run the auto tuning. Please add the build directory of TensileLite to `PATH` environment variable or create an alias `Tensile.sh` that points to the absolute path of `Tensile.sh`.

```shell
confgen tensile --help
```

The usage of this subcommand is completely the same as `Tensile.sh`. For example:

```shell
confgen tensile config.yaml output/
```

### End-to-end Config Generation and Benchmarking

The `autotune` command combines LLM-based config generation and Tensile benchmarking into a single workflow. View the full command options:

```shell
confgen autotune --help
```

Run autotune with default settings (gpt-oss:120b model, MI210 GPU):

```shell
confgen autotune config.yaml output/
```

This will:
1. Generate optimized config using LLM
2. Save modified config to `output/modified.yaml`
3. Automatically run Tensile benchmark with the generated config

Run `autotune` with a specific LLM model:

```shell
confgen autotune --model gpt-oss:120b config.yaml output/
```

Run `autotune` with validation against baseline Tensile results:

```shell
confgen autotune config.yaml output/ --validate golden/
```

This runs both LLM-optimized tuning and baseline Tensile tuning, then compares the results to verify that the LLM-generated configuration produces equivalent or better performance. The baseline results are written to the golden/ directory for comparison.
